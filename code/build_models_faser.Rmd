---
title: "nonwear"
author: "Esben Lykke"
date: "4/11/2021"
output: html_document:
  theme:
    bootswatch: journal
    keep_md: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  message = FALSE,
  warning = FALSE
)
  
pacman::p_load(
  tidyverse,
  here,
  doParallel,
  tictoc,
  tidymodels
)

ctrl <-
  control_grid(
    save_pred = FALSE,
    save_workflow = TRUE,
    verbose = TRUE,
    allow_par = TRUE,
    parallel_over = "everything"
  )

my_metrics <-
  metric_set(f_meas, accuracy)

tidymodels_prefer()
options(tidymodels.dark = TRUE)

conflicted::conflict_prefer("spec", "yardstick")


all_cores <- detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

# Read faser data from rds
```{r}
faser_nw <-
  read_rds(here("data", "faser_full.rds"))
```


Partition data into train and test
```{r}
set.seed(123)
spl <-
  faser_nw %>% 
  initial_time_split(prop = 0.8071749)

train <-
  training(spl)

test <-
  testing(spl)

folds <-
  vfold_cv(train, strata = wear_criterion, v = 5)
```


Decision tree specification
```{r}
tree_mod <-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

# Full tree
```{r}
tree_full_rec <-
  recipe(wear_criterion ~ ., data = train) %>%
  update_role(id, new_role = "id") %>%
  # TODO consider SMOTE to address sligth class imbalance.
  # themis::step_smote(all_outcomes()) %>%
  step_naomit(all_predictors(), skip = TRUE) 

tree_wf <-
  workflow(tree_full_rec, tree_mod)

tree_grid <-
  grid_latin_hypercube(
    cost_complexity(),
    tree_depth(),
    min_n(),
    size = 10
  )

tic(msg = "Tune_grid tager:")
tree_rs <-
  tune_grid(tree_wf,
    folds,
    control = ctrl,
    grid = tree_grid,
    metrics = my_metrics
  )
toc(log = TRUE)

beepr::beep(8)

# write_rds(tree_rs, "tree_full_new.RDS")
```

# Tree imp6
```{r}
tree_imp6_rec <-
  recipe(wear_criterion ~ id + temp + sdmax + sdacc_x + sdacc_y + sdacc_z + macc_z, data = train) %>%
  update_role(id, new_role = "id") %>%
  # TODO consider SMOTE to address sligth class imbalance.
  # themis::step_smote(all_outcomes()) %>%
  step_naomit(all_predictors(), skip = TRUE)

tree_imp6_wf <-
  workflow(tree_imp6_rec, tree_mod)

tree_grid <-
  grid_latin_hypercube(
    cost_complexity(),
    tree_depth(),
    min_n(),
    size = 10
  )

tic(msg = "Tune_grid tager:")
tree_rs_imp6 <-
  tune_grid(tree_wf,
    folds,
    control = ctrl,
    grid = tree_grid,
    metrics = my_metrics
  )
toc(log = TRUE)

beepr::beep(4)

# write_rds(tree_rs, "tree_rs_imp6.RDS")
```

# No temp tree
```{r}
tree_no_temp_rec <-
  recipe(wear_criterion ~ id + sdmax + sdacc_x + sdacc_y + sdacc_z + macc_z + macc_y + macc_x + incl + time_day + location + weekday, data = train) %>%
  update_role(id, new_role = "id") %>%
  # TODO consider SMOTE to address sligth class imbalance.
  # themis::step_smote(all_outcomes()) %>%
  step_naomit(all_predictors(), skip = TRUE)

tree_no_temp_wf <-
  workflow(tree_no_temp_rec, tree_mod)

set.seed(1234)
tree_grid <-
  grid_latin_hypercube(
    cost_complexity(),
    tree_depth(),
    min_n(),
    size = 10
  )

tic(msg = "Tune_grid tager:")
tree_no_temp_rs <-
  tune_grid(tree_no_temp_wf,
    folds,
    control = ctrl,
    grid = tree_grid,
    metrics = metric_set(f_meas)
  )
toc(log = TRUE)

write_rds(tree_no_temp_rs, "tree_no_temp_rs.RDS")

tree_no_temp_fit <-
  tree_no_temp_wf %>%
  finalize_workflow(select_best(tree_no_temp_rs, "f_meas")) %>%
  fit(train)

# write_rds(tree_no_temp_fit, here("models", "tree_no_temp_fit.rds"))

beepr::beep(4)
```

# No temp top 5 imp tree
```{r}
tree_no_temp_imp5_rec <-
  recipe(wear_criterion ~ id + sdmax + sdacc_x + sdacc_y + sdacc_z + macc_z, data = train) %>%
  update_role(id, new_role = "id") %>%
  # TODO consider SMOTE to address sligth class imbalance.
  # themis::step_smote(all_outcomes()) %>%
  step_naomit(all_predictors(), skip = TRUE)

tree_no_temp_imp5_wf <-
  workflow(tree_no_temp_imp5_rec, tree_mod)

set.seed(1234)
tree_grid <-
  grid_latin_hypercube(
    cost_complexity(),
    tree_depth(),
    min_n(),
    size = 10
  )

tic(msg = "Tune_grid tager:")
tree_no_temp_imp5_rs <-
  tune_grid(tree_no_temp_imp5_wf,
    folds,
    control = ctrl,
    grid = tree_grid,
    metrics = metric_set(f_meas)
  )
toc(log = TRUE)

beepr::beep(4)

write_rds(tree_no_temp_imp5_rs, "tree_no_temp_imp5_rs.RDS")

tree_no_temp_imp5_fit <-
  tree_no_temp_imp5_wf %>%
  finalize_workflow(select_best(tree_no_temp_imp5_rs, "f_meas")) %>%
  fit(train)

write_rds(tree_no_temp_imp5_fit, here("models", "tree_no_temp_imp5_fit.rds"))

beepr::beep(4)

```


# Workflowset of tree models (experimental)
```{r}
# TODO as_workflow_sets() !!!
# tree_models <- 
#   workflow_set(preproc = list(full = tree_full_rec,
#                               imp6 = tree_imp6_rec,
#                               no_temp = tree_no_temp_rec,
#                               no_temp_imp5 = tree_no_temp_imp5_rec),
#                models = list(tree = tree_mod), cross = FALSE)
# 
# tree_models_rs <- 
#   workflow_map("tune_grid", 
#                verbose = TRUE,
#                seed = 123, 
#                resamples = folds,
#                control = ctrl,
#                grid = tree_grid)

# TODO leave_var_out_formulas()
```

# Logistic regressiom
Tune glm hyperparameters
```{r}
glm_mod <-
  logistic_reg(
    penalty = tune(),
    mixture = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

glm_rec <-
  recipe(wear_criterion ~ ., data = train) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors(), -all_nominal()) %>%
  update_role(id, new_role = "id") %>%
  step_naomit(all_predictors(), skip = TRUE)

glm_wf <-
  workflow(glm_rec, glm_mod)

glm_grid <-
  grid_max_entropy(
    penalty(),
    mixture(),
    size = 10
  )


glm_rs <-
  tune_grid(glm_wf,
    folds,
    control = ctrl,
    grid = glm_grid,
    metrics = my_metrics
  )

beepr::beep(3)

# saveRDS(glm_rs, here("glm_rs.rds"))

# Finalize glm_wf and fit to all training data
glm_fit <-
  glm_wf %>%
  finalize_workflow(select_best(glm_rs, "f_meas")) %>%
  fit(train)

# write_rds(glm_fit, here("models", "glm_fit.rds"))
```
